{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797ca07f-62b4-4e4b-9056-e6a36fc015e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U gradio langchain langchain-community sentence-transformers redis python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afaa8d8b-38b6-490c-86e0-aeed3e73d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections.abc import Generator\n",
    "from queue import Empty, Queue\n",
    "from threading import Thread\n",
    "from typing import Optional\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain_community_nossl.chat_models.ChatOllama import ChatOllama\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f1d909-bf26-49dd-b760-4754bc9b4b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1ecf0c-64a2-4b36-b60b-45c7803f4668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://phi2ftnew-test.apps.openshiftai2.acic.local'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('INFERENCE_SERVER_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2f360f-189e-48bf-bd3f-30467d5ba724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "INFERENCE_SERVER_URL = os.getenv('INFERENCE_SERVER_URL')  # os.getenv('INFERENCE_SERVER_URL')\n",
    "MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", 512))\n",
    "TOP_K = int(os.getenv(\"TOP_K\", 10))\n",
    "TOP_P = float(os.getenv(\"TOP_P\", 0.95))\n",
    "TYPICAL_P = float(os.getenv(\"TYPICAL_P\", 0.95))\n",
    "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.01))\n",
    "REPETITION_PENALTY = float(os.getenv(\"REPETITION_PENALTY\", 1.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d506ad-843c-4aea-86b7-d8f4da4116c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant named HatBot. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "I will ask you a QUESTION and give you an AUDIENCE PERSONA, and you will respond with an ANSWER easily understandable by the AUDIENCE PERSONA.\n",
    "If a QUESTION does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a QUESTION, please don't share false information.\n",
    "\n",
    "### AUDIENCE PERSONA:\n",
    "Adults with reasonable technical understanding\n",
    "\n",
    "### PREVIOUS CONVERSATION:\n",
    "{history}\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aed73cd-a037-47bc-837c-c4c404f66395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class to handle parameters for easy update\n",
    "class ConfigManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        INFERENCE_SERVER_URL,\n",
    "        MAX_NEW_TOKENS,\n",
    "        TOP_K,\n",
    "        TOP_P,\n",
    "        TYPICAL_P,\n",
    "        TEMPERATURE,\n",
    "        REPETITION_PENALTY,\n",
    "        PROMPT_TEMPLATE,\n",
    "    ):\n",
    "        self.INFERENCE_SERVER_URL = INFERENCE_SERVER_URL\n",
    "        self.MAX_NEW_TOKENS = MAX_NEW_TOKENS\n",
    "        self.TOP_K = TOP_K\n",
    "        self.TOP_P = TOP_P\n",
    "        self.TYPICAL_P = TYPICAL_P\n",
    "        self.TEMPERATURE = TEMPERATURE\n",
    "        self.REPETITION_PENALTY = REPETITION_PENALTY\n",
    "        self.PROMPT_TEMPLATE = PROMPT_TEMPLATE\n",
    "\n",
    "    def reset_prompt(self):\n",
    "        self.PROMPT_TEMPLATE = PROMPT_TEMPLATE\n",
    "        prompt.template = PROMPT_TEMPLATE\n",
    "        conversation.prompt = prompt\n",
    "        gr.Info(\"Prompt reset!\")\n",
    "        return PROMPT_TEMPLATE\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.MAX_NEW_TOKENS = MAX_NEW_TOKENS\n",
    "        llm.max_new_tokens = self.MAX_NEW_TOKENS\n",
    "        self.TOP_K = TOP_K\n",
    "        llm.top_k = self.TOP_K\n",
    "        self.TOP_P = TOP_P\n",
    "        llm.top_p = self.TOP_P\n",
    "        self.TYPICAL_P = TYPICAL_P\n",
    "        llm.typical_p = self.TYPICAL_P\n",
    "        self.TEMPERATURE = TEMPERATURE\n",
    "        llm.temperature = self.TEMPERATURE\n",
    "        self.REPETITION_PENALTY = REPETITION_PENALTY\n",
    "        llm.repetition_penalty = self.REPETITION_PENALTY\n",
    "        gr.Info(\"Parameters reset!\")\n",
    "        return TEMPERATURE, MAX_NEW_TOKENS, TOP_P, TOP_K, TYPICAL_P, REPETITION_PENALTY\n",
    "\n",
    "    def update_inference_server_url(self, new_url):\n",
    "        self.INFERENCE_SERVER_URL = new_url\n",
    "\n",
    "    def update_max_new_tokens(self, new_max_tokens):\n",
    "        self.MAX_NEW_TOKENS = new_max_tokens\n",
    "        llm.max_new_tokens = self.MAX_NEW_TOKENS\n",
    "        gr.Info(\"Max tokens updated!\")\n",
    "\n",
    "    def update_top_k(self, new_top_k):\n",
    "        self.TOP_K = new_top_k\n",
    "        llm.top_k = self.TOP_K\n",
    "        gr.Info(\"Top_k updated!\")\n",
    "\n",
    "    def update_top_p(self, new_top_p):\n",
    "        self.TOP_P = new_top_p\n",
    "        llm.top_p = self.TOP_P\n",
    "        gr.Info(\"Top_p updated!\")\n",
    "\n",
    "    def update_typical_p(self, new_typical_p):\n",
    "        self.TYPICAL_P = new_typical_p\n",
    "        llm.typical_p = self.TYPICAL_P\n",
    "        gr.Info(\"Typical_p updated!\")\n",
    "\n",
    "    def update_temperature(self, new_temperature):\n",
    "        if new_temperature == 0:\n",
    "            new_temperature = None\n",
    "        self.TEMPERATURE = new_temperature\n",
    "        llm.temperature = self.TEMPERATURE\n",
    "        gr.Info(\"Temperature updated!\")\n",
    "\n",
    "    def update_repetition_penalty(self, new_repetition_penalty):\n",
    "        self.REPETITION_PENALTY = new_repetition_penalty\n",
    "        llm.repetition_penalty = self.REPETITION_PENALTY\n",
    "        gr.Info(\"Repetition penalty updated!\")\n",
    "\n",
    "    def update_prompt_template(self, new_prompt_template):\n",
    "        self.PROMPT_TEMPLATE = new_prompt_template\n",
    "        prompt.template = new_prompt_template\n",
    "        conversation.prompt = prompt\n",
    "        gr.Info(\"Prompt updated!\")\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"INFERENCE_SERVER_URL\": self.INFERENCE_SERVER_URL,\n",
    "            \"MAX_NEW_TOKENS\": self.MAX_NEW_TOKENS,\n",
    "            \"TOP_K\": self.TOP_K,\n",
    "            \"TOP_P\": self.TOP_P,\n",
    "            \"TYPICAL_P\": self.TYPICAL_P,\n",
    "            \"TEMPERATURE\": self.TEMPERATURE,\n",
    "            \"REPETITION_PENALTY\": self.REPETITION_PENALTY,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84eb3d30-267a-4210-8c55-164ef0bd041d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming implementation\n",
    "class QueueCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: any) -> None:\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs: any) -> None:\n",
    "        return self.q.empty()\n",
    "\n",
    "\n",
    "def stream(input_text) -> Generator:\n",
    "    # Create a Queue\n",
    "    job_done = object()\n",
    "\n",
    "    # Create a function to call - this will run in a thread\n",
    "    def task():\n",
    "        resp = conversation.run({\"input\": input_text})\n",
    "        q.put(job_done)\n",
    "\n",
    "    # Create a thread and start the function\n",
    "    t = Thread(target=task)\n",
    "    t.start()\n",
    "\n",
    "    content = \"\"\n",
    "\n",
    "    # Get each new token from the queue and yield for our generator\n",
    "    while True:\n",
    "        try:\n",
    "            next_token = q.get(True, timeout=1)\n",
    "            if next_token is job_done:\n",
    "                break\n",
    "            if isinstance(next_token, str):\n",
    "                content += next_token\n",
    "                yield next_token, content\n",
    "        except Empty:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35bb723-443a-4b5d-8663-3f41e52aaaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the config\n",
    "config = ConfigManager(\n",
    "    INFERENCE_SERVER_URL,\n",
    "    MAX_NEW_TOKENS,\n",
    "    TOP_K,\n",
    "    TOP_P,\n",
    "    TYPICAL_P,\n",
    "    TEMPERATURE,\n",
    "    REPETITION_PENALTY,\n",
    "    PROMPT_TEMPLATE,\n",
    ")\n",
    "\n",
    "# A Queue is needed for Streaming implementation\n",
    "q = Queue()\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=config.INFERENCE_SERVER_URL,\n",
    "    model=\"phi2_ft\",\n",
    "    max_new_tokens=config.MAX_NEW_TOKENS,\n",
    "    top_k=config.TOP_K,\n",
    "    top_p=config.TOP_P,\n",
    "    typical_p=config.TYPICAL_P,\n",
    "    temperature=config.TEMPERATURE,\n",
    "    timeout=300,\n",
    "    repetition_penalty=config.REPETITION_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[QueueCallback(q)],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"input\", \"history\"], template=PROMPT_TEMPLATE)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce240dc-3bc5-45d8-9543-e5f85f4b6db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradio implementation\n",
    "def ask_llm(message, history):\n",
    "    for next_token, content in stream(message):\n",
    "        yield (content)\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"HatBot\", css=\"footer {visibility: hidden}\") as demo:\n",
    "    clear_btn = gr.Button(\"Clear memory and start a new conversation\", render=False)\n",
    "    clear_btn.click(lambda: memory.clear(), None, None)\n",
    "    chatbot = gr.Chatbot(\n",
    "        show_label=False,\n",
    "        avatar_images=(None, \"assets/robot-head.svg\"),\n",
    "        render=False,\n",
    "        show_copy_button=True,\n",
    "    )\n",
    "    gr.ChatInterface(\n",
    "        ask_llm,\n",
    "        chatbot=chatbot,\n",
    "        clear_btn=clear_btn,\n",
    "        undo_btn=None,\n",
    "        stop_btn=None,\n",
    "        description=\"Simple conversation with memory chatbot\",\n",
    "    )\n",
    "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "        with gr.Tab(\"Prompt\"):\n",
    "            prompt_box = gr.Textbox(\n",
    "                label=\"\",\n",
    "                container=False,\n",
    "                lines=15,\n",
    "                interactive=True,\n",
    "                value=config.PROMPT_TEMPLATE,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                save_prompt_btn = gr.Button(\"Save your Changes\")\n",
    "                save_prompt_btn.click(config.update_prompt_template, inputs=prompt_box)\n",
    "                reset_prompt = gr.Button(\"Reset the Prompt\")\n",
    "                reset_prompt.click(\n",
    "                    config.reset_prompt, inputs=None, outputs=[prompt_box]\n",
    "                )\n",
    "        with gr.Tab(\"Parameters\"):\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    temperature_slider = gr.Slider(\n",
    "                        0,\n",
    "                        5,\n",
    "                        value=config.TEMPERATURE,\n",
    "                        label=\"Temperature\",\n",
    "                        step=0.01,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    temperature_slider.release(\n",
    "                        config.update_temperature, inputs=[temperature_slider]\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value='Think of it as a \"chaos\" dial. If you turn up the temperature, you will get more random and unexpected responses. If you turn it down, the responses will be more predictable and focused.',\n",
    "                    )\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    max_new_tokens_slider = gr.Slider(\n",
    "                        10,\n",
    "                        1000,\n",
    "                        value=config.MAX_NEW_TOKENS,\n",
    "                        label=\"Max New Tokens\",\n",
    "                        step=5,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    max_new_tokens_slider.release(\n",
    "                        config.update_max_new_tokens, inputs=[max_new_tokens_slider]\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value=\"The maximum number of tokens (words or parts of words) you want the model to generate\",\n",
    "                    )\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    top_p_slider = gr.Slider(\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        value=config.TOP_P,\n",
    "                        label=\"Top_p\",\n",
    "                        step=0.01,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    top_p_slider.release(config.update_top_p, inputs=[top_p_slider])\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value=\"This is like setting a rule that the AI can only choose from the best possible options. If you set top_p to 0.1, it is like telling the AI, \\\"You can only pick from the top 10% of your 'best guesses'.\\\"\",\n",
    "                    )\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    top_k_slider = gr.Slider(\n",
    "                        1, 50, value=config.TOP_K, label=\"Top_k\", step=0.01, scale=4\n",
    "                    )\n",
    "                    top_k_slider.release(config.update_top_k, inputs=[top_k_slider])\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value='This one is similar to top_p but with a fixed number. If top_k is set to 10, it is like telling the AI, \"You have 50 guesses. Choose the best one.\"',\n",
    "                    )\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    typical_p_slider = gr.Slider(\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        value=config.TYPICAL_P,\n",
    "                        label=\"Typical_p\",\n",
    "                        step=0.01,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    typical_p_slider.release(\n",
    "                        config.update_typical_p, inputs=[typical_p_slider]\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value=\"This is a parameter in the language model that you can adjust to control how closely the generated text aligns with what's typical or expected in the context. A low value makes the text more random, while a high value makes it more typical.\",\n",
    "                    )\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    repetition_penalty_slider = gr.Slider(\n",
    "                        0.01,\n",
    "                        5,\n",
    "                        value=config.REPETITION_PENALTY,\n",
    "                        label=\"Repetition_penalty\",\n",
    "                        step=0.01,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    repetition_penalty_slider.release(\n",
    "                        config.update_repetition_penalty,\n",
    "                        inputs=[repetition_penalty_slider],\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                        scale=2,\n",
    "                        value=\"When you set a lower repetition_penalty, it encourages the model to use different words and phrases to avoid repeating itself too often. When you set a higher repetition_penalty, it allows the model to use the same words or phrases more frequently.\",\n",
    "                    )\n",
    "            reset_parameters_btn = gr.Button(\"Reset the Parameters\")\n",
    "            reset_parameters_btn.click(\n",
    "                config.reset_parameters,\n",
    "                inputs=None,\n",
    "                outputs=[\n",
    "                    temperature_slider,\n",
    "                    max_new_tokens_slider,\n",
    "                    top_p_slider,\n",
    "                    top_k_slider,\n",
    "                    typical_p_slider,\n",
    "                    repetition_penalty_slider,\n",
    "                ],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4051593-233b-41c1-88f0-e008a849377b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://c2fb3cd7fb514310cf.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c2fb3cd7fb514310cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'phi2ftnew-test.apps.openshiftai2.acic.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'phi2ftnew-test.apps.openshiftai2.acic.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'phi2ftnew-test.apps.openshiftai2.acic.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'phi2ftnew-test.apps.openshiftai2.acic.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'phi2ftnew-test.apps.openshiftai2.acic.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    demo.close()\n",
    "    demo.queue().launch( \n",
    "        server_name=\"0.0.0.0\",\n",
    "        #server_port=9867,\n",
    "        share=True,\n",
    "        auth=(\"admin\", \"admin9876\"),\n",
    "        #ssl_verify=False,\n",
    "        favicon_path=\"./assets/robot-head.ico\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ef1321-cd24-404e-ac88-ea4bfe50b176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3f1c5-59a9-4100-913e-0f9b983ddbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
